{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8615ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facial detection algorithm\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import uniform\n",
    "import pdb\n",
    "\n",
    "test_images_path = \"../data/validare/Validare\"\n",
    "\n",
    "def find_color_values_using_trackbar(frame):\n",
    "\n",
    "    frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    " \n",
    "    def nothing(x):\n",
    "        pass\n",
    "\n",
    "    cv.namedWindow(\"Trackbar\") \n",
    "    cv.createTrackbar(\"LH\", \"Trackbar\", 0, 255, nothing)\n",
    "    cv.createTrackbar(\"LS\", \"Trackbar\", 0, 255, nothing)\n",
    "    cv.createTrackbar(\"LV\", \"Trackbar\", 0, 255, nothing)\n",
    "    cv.createTrackbar(\"UH\", \"Trackbar\", 255, 255, nothing)\n",
    "    cv.createTrackbar(\"US\", \"Trackbar\", 255, 255, nothing)\n",
    "    cv.createTrackbar(\"UV\", \"Trackbar\", 255, 255, nothing)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "\n",
    "        l_h = cv.getTrackbarPos(\"LH\", \"Trackbar\")\n",
    "        l_s = cv.getTrackbarPos(\"LS\", \"Trackbar\")\n",
    "        l_v = cv.getTrackbarPos(\"LV\", \"Trackbar\")\n",
    "        u_h = cv.getTrackbarPos(\"UH\", \"Trackbar\")\n",
    "        u_s = cv.getTrackbarPos(\"US\", \"Trackbar\")\n",
    "        u_v = cv.getTrackbarPos(\"UV\", \"Trackbar\")\n",
    "\n",
    "\n",
    "        l = np.array([l_h, l_s, l_v])\n",
    "        u = np.array([u_h, u_s, u_v])\n",
    "        mask_table_hsv = cv.inRange(frame_hsv, l, u)        \n",
    "\n",
    "        res = cv.bitwise_and(frame, frame, mask=mask_table_hsv)    \n",
    "        cv.imshow(\"Frame\", frame)\n",
    "        cv.imshow(\"Mask\", mask_table_hsv)\n",
    "        cv.imshow(\"Res\", res)\n",
    "\n",
    "        if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "img = cv.imread(test_images_path + \"/0007.jpg\")\n",
    "find_color_values_using_trackbar(img)\n",
    "low_yellow = (0, 0, 173)\n",
    "high_yellow = (217, 90, 255)\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "mask_yellow_hsv = cv.inRange(img_hsv, low_yellow, high_yellow)\n",
    "cv.imshow('img_initial', img)\n",
    "cv.imshow('mask_yellow_hsv', mask_yellow_hsv)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d665886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#various imports\n",
    "from Parameters import *\n",
    "from FacialDetector import *\n",
    "import pdb\n",
    "from Visualize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ed600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "\n",
    "def show_image(title,image):\n",
    "    image=cv.resize(image,(0,0),fx=0.3,fy=0.3)\n",
    "    cv.imshow(title,image)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()   \n",
    "\n",
    "def non_maximal_suppression(image_detections,image_scores,image_size):\n",
    "    x_out_of_bounds = np.where(image_detections[:, 2] > image_size[1])[0]\n",
    "    y_out_of_bounds = np.where(image_detections[:, 3] > image_size[0])[0]\n",
    "    print(x_out_of_bounds, y_out_of_bounds)\n",
    "    image_detections[x_out_of_bounds, 2] = image_size[1]\n",
    "    image_detections[y_out_of_bounds, 3] = image_size[0]\n",
    "    sorted_indices = np.flipud(np.argsort(image_scores))\n",
    "    sorted_image_detections = image_detections[sorted_indices]\n",
    "    sorted_scores = image_scores[sorted_indices]\n",
    "\n",
    "    is_maximal = np.ones(len(image_detections)).astype(bool)\n",
    "    iou_threshold = 0.01\n",
    "\n",
    "\n",
    "    for i in range(len(sorted_image_detections) - 1):\n",
    "        if is_maximal[i] == True:  # don't change to 'is True' because is a numpy True and is not a python True :)\n",
    "            for j in range(i + 1, len(sorted_image_detections)):\n",
    "                if is_maximal[j] == True:  # don't change to 'is True' because is a numpy True and is not a python True :)\n",
    "                    if facial_detector.intersection_over_union(sorted_image_detections[i],sorted_image_detections[j]) > iou_threshold:is_maximal[j] = False\n",
    "                    else:  # verificam daca centrul detectiei este in mijlocul detectiei cu scor mai mare\n",
    "                        c_x = (sorted_image_detections[j][0] + sorted_image_detections[j][2]) / 2\n",
    "                        c_y = (sorted_image_detections[j][1] + sorted_image_detections[j][3]) / 2\n",
    "                        if sorted_image_detections[i][0] <= c_x <= sorted_image_detections[i][2] and \\\n",
    "                                sorted_image_detections[i][1] <= c_y <= sorted_image_detections[i][3]:\n",
    "                            is_maximal[j] = False\n",
    "    return sorted_image_detections[is_maximal], sorted_scores[is_maximal]\n",
    "\n",
    "def get_positive_features():\n",
    "    positive_descriptors = []\n",
    "    for f in os.listdir(positive_examples_path):\n",
    "        if f[:-3] != \"txt\":\n",
    "            files = os.listdir(positive_examples_path+\"/\"+f)\n",
    "\n",
    "            for train_image in files:\n",
    "                img_path = positive_examples_path + \"/\" + f + \"/\" + train_image\n",
    "                img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                img = cv.resize(img,(int(resize_param[1]), int(resize_param[0])))\n",
    "                \n",
    "                features = hog(\n",
    "                    img, \n",
    "                    pixels_per_cell=pixels_cell,\n",
    "                    cells_per_block=cells_block, \n",
    "                    feature_vector=True,\n",
    "                    orientations=orientations,\n",
    "                    block_norm=\"L2\"\n",
    "                )\n",
    "\n",
    "                positive_descriptors.append(features)\n",
    "\n",
    "                features = hog(\n",
    "                    np.fliplr(img), \n",
    "                    pixels_per_cell=pixels_cell,\n",
    "                    cells_per_block=cells_block,\n",
    "                    feature_vector=True,\n",
    "                    orientations=orientations,\n",
    "                    block_norm=\"L2\"\n",
    "                )\n",
    "\n",
    "                positive_descriptors.append(features)\n",
    "\n",
    "    positive_features = np.array(positive_descriptors,dtype=object)\n",
    "    return positive_features\n",
    "\n",
    "def get_negative_features():\n",
    "    num_negative_per_image = 5\n",
    "    negative_descriptors = []\n",
    "\n",
    "    for directory in directories:\n",
    "        annotationFile = train_folder + directory + \"_annotations.txt\"\n",
    "\n",
    "        with open(annotationFile) as f:\n",
    "            inputRow = f.readline()\n",
    "            while inputRow != '':\n",
    "                splitData = inputRow.split()\n",
    "                imgIndex = splitData[0]\n",
    "                img_path = train_folder+directory+\"/\"+imgIndex\n",
    "\n",
    "                facebox = [int(i) for i in splitData[1:5]]\n",
    "                x1,y1,x2,y2 = [int(i) for i in splitData[1:5]] \n",
    "\n",
    "                img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "                num_rows = img.shape[0]\n",
    "                num_cols = img.shape[1]\n",
    "                \n",
    "                x = np.random.randint(low=0, high=num_cols - resize_param[1], size=num_negative_per_image)\n",
    "                y = np.random.randint(low=0, high=num_rows - resize_param[0], size=num_negative_per_image) \n",
    "\n",
    "                key = annotationFile + imgIndex\n",
    "\n",
    "                for idx in range(len(y)):\n",
    "                    patch = img[y[idx]: int(y[idx] + dim_window*resize_factors[0]), x[idx]: int(x[idx] + dim_window*resize_factors[1])]\n",
    "                    patchbox = [x[idx],y[idx],int(x[idx] +dim_window*resize_factors[1]),int(y[idx] + dim_window*resize_factors[0])]\n",
    "                    \n",
    "                    to_add = True\n",
    "                    for facebox in savedFaces[key]:\n",
    "                        res = facial_detector.intersection_over_union(patchbox,facebox)\n",
    "                        if res > 0.05:\n",
    "                            to_add = False\n",
    "    \n",
    "                    if to_add:\n",
    "                        descr = hog(\n",
    "                            patch, \n",
    "                            pixels_per_cell=pixels_cell,\n",
    "                            cells_per_block=cells_block, \n",
    "                            feature_vector=True,\n",
    "                            orientations=orientations,\n",
    "                            block_norm=\"L2\"\n",
    "                        )\n",
    "                        negative_descriptors.append(descr)\n",
    "\n",
    "                    else:\n",
    "                        idx-=1\n",
    "\n",
    "\n",
    "                if(len(negative_descriptors) >= number_negative_examples):\n",
    "                    break\n",
    "\n",
    "                inputRow = f.readline()\n",
    "\n",
    "        if(len(negative_descriptors) >= number_negative_examples):\n",
    "            break\n",
    "\n",
    "    negative_features = np.array(negative_descriptors,dtype=object)\n",
    "    return negative_features\n",
    "\n",
    "\n",
    "detections = None  # array cu toate detectiile pe care le obtinem\n",
    "scores = np.array([])  # array cu toate scorurile pe care le obtinem\n",
    "file_names = np.array([])  # array cu fisiele, in aceasta lista fisierele vor aparea de mai multe ori, pentru fiecare\n",
    "\n",
    "#predict with our classifier\n",
    "def predict_examples(detections,scores,file_names):\n",
    "    test_files = os.listdir(test_images_path)\n",
    "    w = model.coef_.T\n",
    "    bias = model.intercept_[0]\n",
    "    num_test_images = len(test_files)\n",
    "    descriptors_to_return = []\n",
    "\n",
    "    img = cv.imread(train_folder+\"/andy/0001.jpg\",cv.IMREAD_GRAYSCALE)\n",
    "    img = cv.resize(img,resize_param)\n",
    "\n",
    "    feature = hog(\n",
    "        img, \n",
    "        pixels_per_cell=pixels_cell,\n",
    "        cells_per_block=cells_block, \n",
    "        feature_vector=False,\n",
    "        orientations=orientations\n",
    "    )\n",
    "    print(feature.shape)\n",
    "    \n",
    "    \n",
    "    for i, test_image in enumerate(test_files):\n",
    "        print(i)\n",
    "        test_img = cv.imread(test_images_path + \"/\" + test_image, cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image_scores = []\n",
    "        image_detections = []\n",
    "            \n",
    "        low_pink = (0, 0, 173)\n",
    "        high_pink = (217, 110, 255)\n",
    "\n",
    "        pink_hsv = cv.imread(test_images_path + \"/\" + test_image)\n",
    "        pink_hsv = cv.cvtColor(pink_hsv,cv.COLOR_BGR2HSV)\n",
    "        mask_pink = cv.inRange(pink_hsv,low_pink,high_pink)\n",
    "        \n",
    "        for factor in [0.25,0.33,0.5,0.75,1,1.25,1.5,2]:\n",
    "            img = cv.resize(test_img,(0,0),fx=factor,fy=factor)\n",
    "            \n",
    "            hog_descriptors = hog(\n",
    "                img, \n",
    "                pixels_per_cell=pixels_cell,\n",
    "                cells_per_block=cells_block, \n",
    "                feature_vector=False,\n",
    "                orientations=orientations\n",
    "            )\n",
    "\n",
    "            num_cols = img.shape[1] // dim_hog_cell - 1\n",
    "            num_rows = img.shape[0] // dim_hog_cell - 1\n",
    "            f1 = resize_param[0] // dim_hog_cell - (cells_block[0]-1)\n",
    "            f2 = resize_param[1] // dim_hog_cell - (cells_block[1]-1)\n",
    "\n",
    "            for y in range(0, num_rows - f1):\n",
    "                for x in range(0, num_cols - f2):\n",
    "                    if hog_descriptors[y:y + f1, x:x + f2].flatten().shape != feature.flatten().shape:\n",
    "                        continue\n",
    "                        \n",
    "                    x_min = int(x * dim_hog_cell*(1//factor))\n",
    "                    y_min = int(y * dim_hog_cell*(1//factor))\n",
    "                    x_max = int((x * dim_hog_cell + resize_param[1])*1//factor)\n",
    "                    y_max = int((y * dim_hog_cell + resize_param[0])*1//factor)\n",
    "                    \n",
    "                    if mask_pink[y_min:y_max,x_min:x_max].mean() < 70:\n",
    "                        continue\n",
    "                        \n",
    "                    descr = hog_descriptors[y:y + f1, x:x + f2].flatten()\n",
    "                    score = np.dot(descr, w)[0] + bias\n",
    "                    \n",
    "                    if score > threshold and x_min>10 and y_min>10:\n",
    "                        image_detections.append([x_min, y_min, x_max, y_max])\n",
    "                        image_scores.append(score)\n",
    "\n",
    "\n",
    "        if len(image_scores) > 0:\n",
    "            image_detections, image_scores = non_maximal_suppression(np.array(image_detections),\n",
    "                                                                          np.array(image_scores), img.shape)\n",
    "        if len(image_scores) > 0:\n",
    "            if detections is None:\n",
    "                detections = image_detections\n",
    "            else:\n",
    "                detections = np.concatenate((detections, image_detections))\n",
    "            scores = np.append(scores, image_scores)\n",
    "            short_name = ntpath.basename(test_files[i])\n",
    "            image_names = [short_name for ww in range(len(image_scores))]\n",
    "            file_names = np.append(file_names, image_names)\n",
    "                \n",
    "    return detections,scores,file_names\n",
    "\n",
    "def get_examples():\n",
    "    images_for_negative_examples = []\n",
    "\n",
    "    # Get examples\n",
    "    directories = [\"andy\",\"louie\",\"ora\",\"tommy\"]\n",
    "\n",
    "    train_folder = \"../data/antrenare/\"\n",
    "    positiveExamplesDir = \"../data/exemplePozitive/\"\n",
    "    negativeExamplesDir = \"../data/exempleNegative/\"\n",
    "\n",
    "    savedFaces = {}\n",
    "\n",
    "    for directory in directories:\n",
    "        annotationFile = train_folder + directory + \"_annotations.txt\"\n",
    "        with open(annotationFile) as f:\n",
    "            inputRow = f.readline()\n",
    "\n",
    "            while inputRow != '':\n",
    "                splitData = inputRow.split()\n",
    "                imgIndex = splitData[0]\n",
    "\n",
    "                imgPath = train_folder+directory+\"/\"+imgIndex\n",
    "                img = cv.imread(imgPath)\n",
    "                x1,y1,x2,y2 = [int(i) for i in splitData[1:5]] \n",
    "                positiveExample = img[y1:y2,x1:x2]\n",
    "\n",
    "                saveImgPath = positiveExamplesDir+directory\n",
    "                if not os.path.exists(saveImgPath):\n",
    "                    os.makedirs(saveImgPath)  \n",
    "\n",
    "                #save positive examples\n",
    "                imgToSave = positiveExamplesDir+directory+\"/\"+splitData[5]+\"\".join(splitData[1:4])+imgIndex\n",
    "                cv.imwrite(imgToSave,positiveExample)\n",
    "\n",
    "                key = annotationFile + imgIndex\n",
    "                if key not in savedFaces:\n",
    "                    savedFaces[key] = [[x1,y1,x2,y2]]\n",
    "                else:\n",
    "                    savedFaces[key].append([x1,y1,x2,y2])\n",
    "\n",
    "                #save as negative example\n",
    "                images_for_negative_examples.append(imgPath)\n",
    "\n",
    "                inputRow = f.readline()\n",
    "    return savedFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17399cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup cell\n",
    "params: Parameters = Parameters()\n",
    "facial_detector: FacialDetector = FacialDetector(params)\n",
    "    \n",
    "base_dir = \"../data\"\n",
    "positive_examples_path = base_dir + \"/exemplePozitive\"\n",
    "negative_examples_path = base_dir + \"/exempleNegative\"\n",
    "test_images_path = base_dir + \"/validare/Validare\"\n",
    "\n",
    "# Get examples\n",
    "directories = [\"andy\",\"louie\",\"ora\",\"tommy\"]\n",
    "\n",
    "train_folder = \"../data/antrenare/\"\n",
    "positiveExamplesDir = \"../data/exemplePozitive/\"\n",
    "negativeExamplesDir = \"../data/exempleNegative/\"\n",
    "\n",
    "detections = None\n",
    "scores = np.array([])\n",
    "file_names = np.array([])\n",
    "\n",
    "rectup_detections = None\n",
    "rectup_scores = np.array([])\n",
    "rectup_file_names = np.array([])\n",
    "\n",
    "rectdown_detections = None\n",
    "rectdown_scores = np.array([])\n",
    "rectdown_file_names = np.array([])\n",
    "\n",
    "# parameters\n",
    "\n",
    "dim_hog_cell = 8\n",
    "dim_window = 72\n",
    "overlap = 0.3\n",
    "number_positive_examples = 7236\n",
    "number_negative_examples = 30000\n",
    "has_annotations = False\n",
    "threshold = 0\n",
    "cells_block = (3,3)\n",
    "orientations=22\n",
    "pixels_cell = (dim_hog_cell,dim_hog_cell)\n",
    "resize_factors = [1,1]\n",
    "resize_param = (int(dim_window*resize_factors[0]),int(dim_window*resize_factors[1]))\n",
    "iterate_factors = [[1,0.8],[0.9,1]]\n",
    "\n",
    "for elem in iterate_factors:\n",
    "    \n",
    "    resize_factors = elem\n",
    "    resize_param = (int(dim_window*resize_factors[0]),int(dim_window*resize_factors[1]))\n",
    "    \n",
    "    savedFaces = get_examples()    \n",
    "    \n",
    "    positive_features = get_positive_features()\n",
    "    print(positive_features.shape)\n",
    "    negative_features = get_negative_features()\n",
    "    print(negative_features.shape)\n",
    "    \n",
    "    training_examples = np.concatenate((np.squeeze(positive_features), np.squeeze(negative_features)), axis=0)\n",
    "    train_labels = np.concatenate((np.ones(positive_features.shape[0]), np.zeros(negative_features.shape[0])))\n",
    "    # model = train_classifier(training_examples, train_labels)\n",
    "\n",
    "    model = LinearSVC(\n",
    "        C=10,\n",
    "        max_iter=100000,\n",
    "        random_state=0,\n",
    "        dual = True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.fit(training_examples, train_labels)\n",
    "    acc = model.score(training_examples, train_labels)\n",
    "    print(acc)\n",
    "    \n",
    "    detections = None  # array cu toate detectiile pe care le obtinem\n",
    "    scores = np.array([])  # array cu toate scorurile pe care le obtinem\n",
    "    file_names = np.array([])  # array cu fisiele, in aceasta lista fisierele vor aparea de mai multe ori, pentru fiecare\n",
    "\n",
    "    if resize_factors[1] == 1:\n",
    "        rectdown_detections,rectdown_scores,rectdown_file_names = predict_examples(detections,scores,file_names)\n",
    "    else:\n",
    "        rectup_detections,rectup_scores,rectup_file_names = predict_examples(detections,scores,file_names)\n",
    "    \n",
    "    del positive_features\n",
    "    del negative_features\n",
    "    del model\n",
    "\n",
    "        \n",
    "detections = None\n",
    "scores = np.array([])\n",
    "file_names = np.array([])\n",
    "\n",
    "test_images_path = \"../data/validare/Validare/\"\n",
    "\n",
    "for test_file in os.listdir(test_images_path):\n",
    "    imgpath = test_images_path+\"/\"+test_file\n",
    "    image = cv.imread(imgpath)\n",
    "    short_file_name = ntpath.basename(test_file)\n",
    "    \n",
    "    indices_detections_current_image_up = np.where(rectup_file_names == short_file_name)\n",
    "    indices_detections_current_image_down = np.where(rectdown_file_names == short_file_name)\n",
    "    \n",
    "    saved_rectup_detections = rectup_detections[indices_detections_current_image_up]\n",
    "    saved_rectup_scores = rectup_scores[indices_detections_current_image_up]\n",
    "\n",
    "    saved_rectdown_detections = rectdown_detections[indices_detections_current_image_down]\n",
    "    saved_rectdown_scores = rectdown_scores[indices_detections_current_image_down]\n",
    "    \n",
    "    current_detections = np.concatenate((saved_rectup_detections,saved_rectdown_detections))\n",
    "    current_scores = np.concatenate((saved_rectup_scores,saved_rectdown_scores))\n",
    "    \n",
    "    current_detections, current_scores = non_maximal_suppression(current_detections,current_scores,image.shape)\n",
    "\n",
    "    \n",
    "    if detections is None:\n",
    "        detections = current_detections\n",
    "    else:\n",
    "        detections = np.concatenate((detections, current_detections))\n",
    "    \n",
    "    scores = np.append(scores,current_scores)\n",
    "    \n",
    "    short_name = ntpath.basename(test_file)\n",
    "    image_names = [short_name for ww in range(len(current_scores))]\n",
    "    file_names = np.append(file_names, image_names)\n",
    "\n",
    "# facial_detector.eval_detections(detections, scores, file_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208673d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_bboxes = np.loadtxt(params.path_annotations, dtype='str')\n",
    "test_images_path = \"../data/validare/Validare/\"\n",
    "\n",
    "for test_file in os.listdir(test_images_path):\n",
    "    imgpath = test_images_path+\"/\"+test_file\n",
    "    image = cv.imread(imgpath)\n",
    "    short_file_name = ntpath.basename(test_file)\n",
    "    indices_detections_current_image = np.where(file_names == short_file_name)\n",
    "    current_detections = detections[indices_detections_current_image]\n",
    "    current_scores = scores[indices_detections_current_image]\n",
    "\n",
    "    for idx, detection in enumerate(current_detections):\n",
    "\n",
    "        cv.rectangle(image, (detection[0], detection[1]), (detection[2], detection[3]), (0, 0, 255), thickness=1)\n",
    "        cv.putText(image, 'score:' + str(current_scores[idx])[:4], (detection[0], detection[1]),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    annotations = ground_truth_bboxes[ground_truth_bboxes[:, 0] == short_file_name]\n",
    "\n",
    "    # show ground truth bboxes\n",
    "    for detection in annotations:\n",
    "        cv.rectangle(image, (int(detection[1]), int(detection[2])), (int(detection[3]), int(detection[4])), (0, 255, 0), thickness=1)\n",
    "\n",
    "    cv.imwrite(os.path.join(params.dir_save_files, \"detections_\" + short_file_name), image)\n",
    "    print('Apasa orice tasta pentru a continua...')\n",
    "    cv.imshow('image', np.uint8(image))\n",
    "    cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ba68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./detections\",detections)\n",
    "# np.save(\"./scores\",scores)\n",
    "# np.save(\"./file_names\",file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fb4ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC(C = 10, max_iter=100000)\n",
    "\n",
    "\n",
    "dim_hog_cell = 12\n",
    "dim_window = 144\n",
    "overlap = 0.3\n",
    "number_positive_examples = 7236\n",
    "number_negative_examples = 20000\n",
    "has_annotations = False\n",
    "threshold = 0\n",
    "cells_block = (4,4)\n",
    "orientations=20\n",
    "\n",
    "pixels_cell = (dim_hog_cell,dim_hog_cell)\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "for directory in directories:\n",
    "    annotationFile = train_folder + directory + \"_annotations.txt\"\n",
    "    with open(annotationFile) as f:\n",
    "        inputRow = f.readline()\n",
    "\n",
    "        while inputRow != '':\n",
    "            splitData = inputRow.split()\n",
    "            imgIndex = splitData[0]\n",
    "\n",
    "            imgPath = train_folder+directory+\"/\"+imgIndex\n",
    "            img = cv.imread(imgPath,cv.IMREAD_GRAYSCALE)\n",
    "            x1,y1,x2,y2 = [int(i) for i in splitData[1:5]] \n",
    "            positive_example = img[y1:y2,x1:x2]\n",
    "\n",
    "            positive_example = cv.resize(positive_example,(dim_window,dim_window))\n",
    "\n",
    "            hog_descriptors = hog(\n",
    "                positive_example, \n",
    "                pixels_per_cell=pixels_cell,\n",
    "                cells_per_block=cells_block, \n",
    "                feature_vector=True,\n",
    "                orientations=orientations\n",
    "            )\n",
    "\n",
    "            \n",
    "            if splitData[-1] != \"unknown\":\n",
    "                train_features.append(hog_descriptors)\n",
    "                train_labels.append(splitData[-1])\n",
    "\n",
    "\n",
    "            inputRow = f.readline()\n",
    "\n",
    "classifier.fit(train_features,train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a9f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_faces = [\"andy\", \"louie\", \"ora\", \"tommy\"]\n",
    "dict_detections = {}\n",
    "dict_scores = {}\n",
    "dict_file_names = {}\n",
    "predictions = []\n",
    "for test_file in os.listdir(test_images_path):\n",
    "    imgpath = test_images_path+\"/\"+test_file\n",
    "    image = cv.imread(imgpath,cv.IMREAD_GRAYSCALE)\n",
    "    short_file_name = ntpath.basename(test_file)\n",
    "    indices_detections_current_image = np.where(file_names == short_file_name)\n",
    "    current_detections = detections[indices_detections_current_image]\n",
    "    current_scores = scores[indices_detections_current_image]\n",
    "    \n",
    "\n",
    "    for idx, detection in enumerate(current_detections):\n",
    "        if current_scores[idx] < 0:\n",
    "            continue\n",
    "\n",
    "        x1,y1,x2,y2 = detection\n",
    "        face = image[y1:y2,x1:x2]\n",
    "        face = cv.resize(face,(dim_window,dim_window))\n",
    "        hog_descriptors = hog(\n",
    "            face, \n",
    "            pixels_per_cell=pixels_cell,\n",
    "            cells_per_block=cells_block, \n",
    "            feature_vector=True,\n",
    "            orientations=orientations\n",
    "        )\n",
    "        \n",
    "        prediction = classifier.predict(hog_descriptors.reshape(1,-1))\n",
    "\n",
    "\n",
    "        if prediction[0] not in dict_detections:\n",
    "            dict_detections[prediction[0]] = [detection]\n",
    "            dict_scores[prediction[0]] = [current_scores[idx]]\n",
    "            dict_file_names[prediction[0]] = [short_file_name]\n",
    "        else:\n",
    "            dict_detections[prediction[0]].append(detection)\n",
    "            dict_scores[prediction[0]].append(current_scores[idx])\n",
    "            dict_file_names[prediction[0]].append(short_file_name)\n",
    "            \n",
    "        predictions.append([prediction[0],x1,x2,y1,y2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0034ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_path = \"./task1/\"\n",
    "\n",
    "if not os.path.exists(task1_path):\n",
    "    os.mkdir(task1_path)\n",
    "\n",
    "np.save(task1_path+\"detections\",detections)\n",
    "np.save(task1_path+\"scores\",scores)\n",
    "np.save(task1_path+\"file_names\",file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2783368",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_faces = [\"andy\", \"louie\", \"ora\", \"tommy\"]\n",
    "task2_path = \"./task2/\"\n",
    "if not os.path.exists(task2_path):\n",
    "    os.mkdir(task2_path)\n",
    "\n",
    "for key in interest_faces:\n",
    "    np.save(task2_path+\"detections_\"+key,np.array(dict_detections[key]))\n",
    "    np.save(task2_path+\"scores_\"+key,np.array(dict_scores[key]))\n",
    "    np.save(task2_path+\"file_names_\"+key,np.array(dict_file_names[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78088bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_detector.eval_detections(detections,scores,file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4bb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = np.load(\"./task1/detections.npy\")\n",
    "scores = np.load(\"./task1/scores.npy\")\n",
    "file_names = np.load(\"./task1/file_names.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9aa9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
